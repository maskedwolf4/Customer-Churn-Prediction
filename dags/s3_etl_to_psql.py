from airflow import DAG
from airflow.providers.amazon.aws.operators.s3 import S3ListOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.standard.operators.python import PythonOperator
from airflow.sdk.bases.hook import BaseHook
from airflow.models import Variable
from datetime import datetime
import pandas as pd
import sqlalchemy

import os



#### TRANSFORM STEP....
def download_from_s3_and_load(bucket_name, object_key, local_path):
    """Download file from S3 and load to PostgreSQL"""

    
    # Download from S3
    s3_hook = S3Hook(aws_conn_id='S3_Bucket')
    s3_hook.download_file(
        key=object_key,
        bucket_name=bucket_name,
        local_path=local_path,
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )
    
    # Load to PostgreSQL
    conn = BaseHook.get_connection('postgres_default')
    engine = sqlalchemy.create_engine(
        f"postgresql+psycopg2://{conn.login}:{conn.password}@customer-churn-prediction_7a768e-postgres-1:{conn.port}/{conn.schema}"
    )
    
    df = pd.read_csv(local_path)
    df.to_sql(name="customerchurn", con=engine, if_exists="replace", index=False)

      
    # Clean up file after loading
    os.remove(local_path)

# Get bucket name from Airflow Variable
S3_BUCKET = Variable.get("S3_Bucket_name")

# Define the DAG
with DAG(
    dag_id="s3_etl_to_psql",
    schedule=None,
    start_date=datetime(2025, 1, 1),
    catchup=False,
) as dag:
    
    # Extract STEP...
    list_files = S3ListOperator(
        task_id="list_files",
        bucket=S3_BUCKET,
        aws_conn_id="S3_Bucket",
        delimiter="/",
    )
    
    ### TRANSFORM AND LOAD....
    download_and_load_data = PythonOperator(
        task_id="download_and_load_to_sql",
        python_callable=download_from_s3_and_load,
        op_kwargs={
            "bucket_name": S3_BUCKET,
            "object_key": 'raw.csv',
            "local_path": "/tmp/raw.csv"
        }
    )
    
    list_files >> download_and_load_data